{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "import implementations as imp\n",
    "%run pre_processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../resources/train.csv'\n",
    "test_path = '../resources/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tx_1,y_1,ids_1), (tx_2,y_2,ids_2), (tx_3,y_3,ids_3),indexes,parameters = process_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtest_1,ids_1), (xtest_2,ids_2), (xtest_3,ids_3) = process_test(test_path,indexes,parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y,x,w):\n",
    "    y_pred = predict_labels(w,x)\n",
    "    return (y_pred==y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randint\n",
    "def evaluate_stability(y,tx,lambda_,degree):\n",
    "    acc_train = []\n",
    "    acc_test = []  \n",
    "    k_fold = 5\n",
    "\n",
    "    for i in range (40):\n",
    "        seed = randint(1, 40)\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        (ltr,lte,acctr,accte) = cross_validation_ls_sgd(y, tx, k_indices, k_fold, degree)\n",
    "        #(ltr,lte,acctr,accte) = cross_validation_rg(y, tx, k_indices, k_fold, lambda_, degree)\n",
    "        #(ltr,lte,acctr,accte) = cross_validation_ls(y, tx, k_indices, k_fold, degree)\n",
    "        acc_train.append(acctr)\n",
    "        acc_test.append(accte)\n",
    "    \n",
    "    return (np.mean(acc_test),np.std(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best parameters with least squares gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation for least squares gradient descent\n",
    "def cross_validation_ls_sgd(y, tx, k_indices, k_fold , degree):\n",
    "    gamma = 0.01\n",
    "    max_iters = 2 \n",
    "    loss_te = np.array([])\n",
    "    loss_tr = np.array([])\n",
    "    acc_te = np.array([]) \n",
    "    acc_tr = np.array([]) \n",
    "    #print(x.shape)\n",
    "    for i in range(k_fold):\n",
    "        sub = np.delete(k_indices,i,axis=0).flatten()\n",
    "        x_train = tx[sub]\n",
    "        y_train = y[sub]\n",
    "        x_test = tx[k_indices[i]]\n",
    "        y_test = y[k_indices[i]]\n",
    "        \n",
    "        x_train = expand_with_pairwise_products(x_train,degree)\n",
    "        x_test = expand_with_pairwise_products(x_test,degree)\n",
    "        initial_w = np.zeros((x_train.shape[1], 1))\n",
    "        \n",
    "        #x_train = np.append(x_train,np.cos(x_train),axis=1)\n",
    "        #x_train = np.append(x_train,np.sin(x_train),axis=1)\n",
    "        #x_test = np.append(x_test,np.cos(x_test),axis=1)\n",
    "        #x_test = np.append(x_test,np.sin(x_test),axis=1)\n",
    "\n",
    "        w,loss_train = imp.least_squares_GD(y_train, x_train,initial_w, max_iters, gamma)\n",
    "        acc_train = compute_accuracy(y_train,x_train,w)\n",
    "        acc_test = compute_accuracy(y_test,x_test,w)\n",
    "        loss_test = imp.compute_loss(y_test, x_test, w)\n",
    "        loss_tr = np.append(loss_tr,loss_train)\n",
    "        loss_te= np.append(loss_te,loss_test)\n",
    "        acc_tr = np.append(acc_tr,acc_train)\n",
    "        acc_te = np.append(acc_te,acc_test)\n",
    "    \n",
    "    return np.mean(loss_tr),np.mean(loss_te),np.mean(acc_tr), np.mean(acc_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for deg in range (0,5):\n",
    "    meanacc,stdacc = evaluate_stability(y_1,tx_1,1,deg)\n",
    "    print(\"for deg \", deg, \" : mean acc = \", meanacc, \" std acc = \",stdacc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for deg in range (0,5):\n",
    "    meanacc,stdacc = evalutate_stability(y_2,tx_2,1,deg)\n",
    "    print(\"for deg \", deg, \" : mean acc = \", meanacc, \" std acc = \",stdacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for deg in range (0,5):\n",
    "    meanacc,stdacc = evalutate_stability(y_3,tx_3,1,deg)\n",
    "    print(\"for deg \", deg, \" : mean acc = \", meanacc, \" std acc = \",stdacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best results for least squares stochastic gradient descent \n",
    "#batch 1 : deg = 8   avg acc =    std acc = \n",
    "#batch 2 : deg = 12  avg acc =    std acc = \n",
    "#batch 3 : deg = 12  avg acc =    std acc = \n",
    "#overall avc acc = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best parameters with least squares stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation for least squares stochastic gradient descent\n",
    "def cross_validation_ls_sgd(y, tx, k_indices, k_fold , degree):\n",
    "    gamma = 0.01\n",
    "    max_iters = 2\n",
    "    batch_size = 100 \n",
    "    loss_te = np.array([])\n",
    "    loss_tr = np.array([])\n",
    "    acc_te = np.array([]) \n",
    "    acc_tr = np.array([]) \n",
    "    #print(x.shape)\n",
    "    for i in range(k_fold):\n",
    "        sub = np.delete(k_indices,i,axis=0).flatten()\n",
    "        x_train = tx[sub]\n",
    "        y_train = y[sub]\n",
    "        x_test = tx[k_indices[i]]\n",
    "        y_test = y[k_indices[i]]\n",
    "        \n",
    "        x_train = expand_with_pairwise_products(x_train,degree)\n",
    "        x_test = expand_with_pairwise_products(x_test,degree)\n",
    "        initial_w = np.zeros((x_train.shape[1], 1))\n",
    "        \n",
    "        #x_train = np.append(x_train,np.cos(x_train),axis=1)\n",
    "        #x_train = np.append(x_train,np.sin(x_train),axis=1)\n",
    "        #x_test = np.append(x_test,np.cos(x_test),axis=1)\n",
    "        #x_test = np.append(x_test,np.sin(x_test),axis=1)\n",
    "\n",
    "        w,loss_train = imp.least_squares_SGD(y_train, x_train,initial_w, batch_size, max_iters, gamma)\n",
    "        acc_train = compute_accuracy(y_train,x_train,w)\n",
    "        acc_test = compute_accuracy(y_test,x_test,w)\n",
    "        loss_test = imp.compute_loss(y_test, x_test, w)\n",
    "        loss_tr = np.append(loss_tr,loss_train)\n",
    "        loss_te= np.append(loss_te,loss_test)\n",
    "        acc_tr = np.append(acc_tr,acc_train)\n",
    "        acc_te = np.append(acc_te,acc_test)\n",
    "    \n",
    "    return np.mean(loss_tr),np.mean(loss_te),np.mean(acc_tr), np.mean(acc_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for deg  0  : mean acc =  0.7366177059353418  std acc =  0.009624198161589994\n"
     ]
    }
   ],
   "source": [
    "for deg in range (0,5):\n",
    "    meanacc,stdacc = evaluate_stability(y_1,tx_1,1,deg)\n",
    "    print(\"for deg \", deg, \" : mean acc = \", meanacc, \" std acc = \",stdacc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for deg in range (0,5):\n",
    "    meanacc,stdacc = evalutate_stability(y_2,tx_2,1,deg)\n",
    "    print(\"for deg \", deg, \" : mean acc = \", meanacc, \" std acc = \",stdacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for deg in range (0,5):\n",
    "    meanacc,stdacc = evalutate_stability(y_3,tx_3,1,deg)\n",
    "    print(\"for deg \", deg, \" : mean acc = \", meanacc, \" std acc = \",stdacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best results for least squares stochastic gradient descent \n",
    "#batch 1 : deg = 8   avg acc =    std acc = \n",
    "#batch 2 : deg = 12  avg acc =    std acc = \n",
    "#batch 3 : deg = 12  avg acc =    std acc = \n",
    "#overall avc acc = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best parameters with least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation for least squares\n",
    "def cross_validation_ls(y, x, k_indices, k_fold , degree):\n",
    "    loss_te = np.array([])\n",
    "    loss_tr = np.array([])\n",
    "    acc_te = np.array([]) \n",
    "    acc_tr = np.array([]) \n",
    "    #print(x.shape)\n",
    "    for i in range(k_fold):\n",
    "        sub = np.delete(k_indices,i,axis=0).flatten()\n",
    "        x_train = x[sub]\n",
    "        y_train = y[sub]\n",
    "        x_test = x[k_indices[i]]\n",
    "        y_test = y[k_indices[i]]\n",
    "        \n",
    "        x_train = expand_with_pairwise_products(x_train,degree)\n",
    "        x_test = expand_with_pairwise_products(x_test,degree)\n",
    "        \n",
    "        #x_train = np.append(x_train,np.cos(x_train),axis=1)\n",
    "        #x_train = np.append(x_train,np.sin(x_train),axis=1)\n",
    "        #x_test = np.append(x_test,np.cos(x_test),axis=1)\n",
    "        #x_test = np.append(x_test,np.sin(x_test),axis=1)\n",
    "\n",
    "        w,loss_train = imp.least_squares(y_train, x_train)\n",
    "        acc_train = compute_accuracy(y_train,x_train,w)\n",
    "        acc_test = compute_accuracy(y_test,x_test,w)\n",
    "        loss_test = imp.compute_loss(y_test, x_test, w)\n",
    "        loss_tr = np.append(loss_tr,loss_train)\n",
    "        loss_te= np.append(loss_te,loss_test)\n",
    "        acc_tr = np.append(acc_tr,acc_train)\n",
    "        acc_te = np.append(acc_te,acc_test)\n",
    "    \n",
    "    return np.mean(loss_tr),np.mean(loss_te),np.mean(acc_tr), np.mean(acc_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for deg  0  : mean acc =  0.7957191472325092  std acc =  0.00013912521269141684\n",
      "for deg  1  : mean acc =  0.8323631268141328  std acc =  0.00015279399169057867\n",
      "for deg  2  : mean acc =  0.8337558802922631  std acc =  0.00012203090884993148\n",
      "for deg  3  : mean acc =  0.8357466720048041  std acc =  0.0001682429115639703\n",
      "for deg  4  : mean acc =  0.8387889100190172  std acc =  0.00014135044294997072\n",
      "for deg  5  : mean acc =  0.8409643679311379  std acc =  0.00013730502370455237\n",
      "for deg  6  : mean acc =  0.8381423280952858  std acc =  0.014848742056491664\n",
      "for deg  7  : mean acc =  0.8398308477629867  std acc =  0.011566748377691465\n",
      "for deg  8  : mean acc =  0.8450440396356722  std acc =  0.0003718353247391593\n",
      "for deg  9  : mean acc =  0.8415108597737964  std acc =  0.009320020138757627\n",
      "for deg  10  : mean acc =  0.8441447302572316  std acc =  0.002260838641793926\n",
      "for deg  11  : mean acc =  0.7880307276548894  std acc =  0.03329041646415769\n",
      "for deg  12  : mean acc =  0.7738524672204984  std acc =  0.04199541823516789\n",
      "for deg  13  : mean acc =  0.6960139125212692  std acc =  0.04567342937541805\n"
     ]
    }
   ],
   "source": [
    "for deg in range (0,14):\n",
    "    meanacc,stdacc = evalutate_stability(y_1,tx_1,1,deg)\n",
    "    print(\"for deg \", deg, \" : mean acc = \", meanacc, \" std acc = \",stdacc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for deg  0  : mean acc =  0.6699277792107299  std acc =  0.0004526787753867319\n",
      "for deg  1  : mean acc =  0.7513373742584474  std acc =  0.0003422599879055013\n",
      "for deg  2  : mean acc =  0.7724258447253033  std acc =  0.00018265385494497166\n",
      "for deg  3  : mean acc =  0.77662883672943  std acc =  0.00028023269240217346\n",
      "for deg  4  : mean acc =  0.7841842919783337  std acc =  0.00020583163317026163\n",
      "for deg  5  : mean acc =  0.7855384317771474  std acc =  0.0002391357508320971\n",
      "for deg  6  : mean acc =  0.7866397988135156  std acc =  0.00029886878150629776\n",
      "for deg  7  : mean acc =  0.7933460149600207  std acc =  0.0002643474316164753\n",
      "for deg  8  : mean acc =  0.8006686871292235  std acc =  0.00024931690743162536\n",
      "for deg  9  : mean acc =  0.8040624193964406  std acc =  0.0001953734512193018\n",
      "for deg  10  : mean acc =  0.8041726850657724  std acc =  0.00020360554846343565\n",
      "for deg  11  : mean acc =  0.8047517410368842  std acc =  0.00029961911925489556\n",
      "for deg  12  : mean acc =  0.8062819190095436  std acc =  0.00014765868113824493\n",
      "for deg  13  : mean acc =  0.7957357492906887  std acc =  0.02281630676212466\n"
     ]
    }
   ],
   "source": [
    "for deg in range (0,14):\n",
    "    meanacc,stdacc = evalutate_stability(y_2,tx_2,1,deg)\n",
    "    print(\"for deg \", deg, \" : mean acc = \", meanacc, \" std acc = \",stdacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for deg  0  : mean acc =  0.7311421284808381  std acc =  0.0004536973572947324\n",
      "for deg  1  : mean acc =  0.7868341604631928  std acc =  0.0003002731708084146\n",
      "for deg  2  : mean acc =  0.7914826302729528  std acc =  0.00041324946611870876\n",
      "for deg  3  : mean acc =  0.8055459057071961  std acc =  0.0003395852461630336\n",
      "for deg  4  : mean acc =  0.8128522194651226  std acc =  0.0003254983411505178\n",
      "for deg  5  : mean acc =  0.8147077474496831  std acc =  0.00031769216976191905\n",
      "for deg  6  : mean acc =  0.8159043286462643  std acc =  0.0003650190492085563\n",
      "for deg  7  : mean acc =  0.8233478081058724  std acc =  0.00033807580422777153\n",
      "for deg  8  : mean acc =  0.8324965536255858  std acc =  0.00026080106775609464\n",
      "for deg  9  : mean acc =  0.8359753239591949  std acc =  0.00030998293922770574\n",
      "for deg  10  : mean acc =  0.8360366694237662  std acc =  0.0003029957051469666\n",
      "for deg  11  : mean acc =  0.8368927488282327  std acc =  0.00038824229970597744\n",
      "for deg  12  : mean acc =  0.8343148607664738  std acc =  0.009791875569386417\n",
      "for deg  13  : mean acc =  0.8286807278742764  std acc =  0.019480174744884376\n"
     ]
    }
   ],
   "source": [
    "for deg in range (0,14):\n",
    "    meanacc,stdacc = evalutate_stability(y_3,tx_3,1,deg)\n",
    "    print(\"for deg \", deg, \" : mean acc = \", meanacc, \" std acc = \",stdacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best results for least squares\n",
    "#batch 1 : deg = 8   avg acc = 0.8450440396356722   std acc = 0.0003718353247391593\n",
    "#batch 2 : deg = 12  avg acc = 0.8062819190095436   std acc = 0.00014765868113824493\n",
    "#batch 3 : deg = 12  avg acc = 0.8368927488282327   std acc = 0.009791875569386417"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best parameters with ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_rg(y, x, k_indices, k , lambda_, degree):\n",
    "    loss_te = np.array([])\n",
    "    loss_tr = np.array([])\n",
    "    acc_te = np.array([]) \n",
    "    acc_tr = np.array([]) \n",
    "    #print(x.shape)\n",
    "    for i in range(k):\n",
    "        sub = np.delete(k_indices,i,axis=0).flatten()\n",
    "        x_train = x[sub]\n",
    "        y_train = y[sub]\n",
    "        x_test = x[k_indices[i]]\n",
    "        y_test = y[k_indices[i]]\n",
    "        \n",
    "        x_train = expand_with_pairwise_products(x_train,degree)\n",
    "        x_test = expand_with_pairwise_products(x_test,degree)\n",
    "        \n",
    "        #x_train = np.append(x_train,np.cos(x_train),axis=1)\n",
    "        #x_train = np.append(x_train,np.sin(x_train),axis=1)\n",
    "        #x_test = np.append(x_test,np.cos(x_test),axis=1)\n",
    "        #x_test = np.append(x_test,np.sin(x_test),axis=1)\n",
    "\n",
    "        w,loss_train = imp.ridge_regression(y_train, x_train,lambda_)\n",
    "        acc_train = compute_accuracy(y_train,x_train,w)\n",
    "        acc_test = compute_accuracy(y_test,x_test,w)\n",
    "        loss_test = imp.compute_loss(y_test, x_test, w)\n",
    "        loss_tr = np.append(loss_tr,loss_train)\n",
    "        loss_te= np.append(loss_te,loss_test)\n",
    "        acc_tr = np.append(acc_tr,acc_train)\n",
    "        acc_te = np.append(acc_te,acc_test)\n",
    "    \n",
    "    return np.mean(loss_tr),np.mean(loss_te),np.mean(acc_tr), np.mean(acc_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_lambda(y,x, degree):\n",
    "    seed = 12\n",
    "    k_fold = 5\n",
    "    lambdas = np.logspace(-4, 0, 40)\n",
    "    best_lambda_tr = 0\n",
    "    best_lambda_te = 0\n",
    "    best_acc_tr = 0\n",
    "    best_acc_te = 0\n",
    "    val_l_tr = (0,0,0,0)\n",
    "    val_l_te = (0,0,0,0)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss and accuracy of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    accuracy_tr = []\n",
    "    accuracy_te = [] \n",
    "    # cross validation\n",
    "    for lambda_ in lambdas:\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        acc_tr_tmp = []\n",
    "        acc_te_tmp = []\n",
    "        for k in range(k_fold-1):\n",
    "            loss_tr, loss_te,acc_tr,acc_te = cross_validation_rg(y, x, k_indices, k+1, lambda_, degree)\n",
    "            if(acc_tr>best_acc_tr):\n",
    "                best_acc_tr = acc_tr\n",
    "                best_lambda_tr = lambda_\n",
    "                val_l_tr=(loss_tr,loss_te,acc_tr,acc_te)\n",
    "            if(acc_te>best_acc_te):\n",
    "                best_acc_te = acc_te\n",
    "                best_lambda_te = lambda_\n",
    "                val_l_te=(loss_tr,loss_te,acc_tr,acc_te)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "            acc_tr_tmp.append(acc_tr)\n",
    "            acc_te_tmp.append(acc_te)\n",
    "            #print(\"rmse_tr_temp = \",rmse_tr_tmp)\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "        accuracy_tr.append(np.mean(acc_tr_tmp))\n",
    "        accuracy_te.append(np.mean(acc_te_tmp))\n",
    "    #cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    return (best_lambda_te,val_l_te)  \n",
    "    #return (lambdas, rmse_tr,rmse_te)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_overall_param():\n",
    "    for deg in range(8):\n",
    "        best_lambda1,loss_acc_1 = find_best_lambda(y_1,tx_1,deg)\n",
    "        print(\"For batch \" + str(1) +\" and degree : \" + str(deg) + \" The best Test (lambda,(losstr,losste,acctr,accte)) : \"+ str((best_lambda1,loss_acc_1)))\n",
    "    \n",
    "    for deg in range(8):\n",
    "        best_lambda2,loss_acc_2 = find_best_lambda(y_2,tx_2,deg)\n",
    "        print(\"For batch \" + str(2) +\" and degree : \" + str(deg) + \" The best Test (lambda,(losstr,losste,acctr,accte)) : \"+ str((best_lambda2,loss_acc_2)))\n",
    "    \n",
    "    for deg in range(8):\n",
    "        best_lambda3,loss_acc_3 = find_best_lambda(y_3,tx_3,deg)\n",
    "        print(\"For batch \" + str(3) +\" and degree : \" + str(deg) + \" The best Test (lambda,(losstr,losste,acctr,accte)) : \"+ str((best_lambda3,loss_acc_3)))\n",
    "        \n",
    "    return (best_lambda1,loss_acc_1),(best_lambda2,loss_acc_2),(best_lambda3,loss_acc_3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For batch 1 and degree : 0 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0001603718743751331, (0.30029315711432913, 0.30555205116348055, 0.7961821889700731, 0.7954283855469924))\n",
      "For batch 1 and degree : 1 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0002030917620904735, (0.2388037549936008, 0.24187571795288057, 0.8330497447702931, 0.8323240916825143))\n",
      "For batch 1 and degree : 2 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.00012663801734674035, (0.2366960779033006, 0.23970574044351156, 0.8342664648183366, 0.8334000600540485))\n",
      "For batch 1 and degree : 3 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0005223345074266843, (0.23388532876818713, 0.7602697967947677, 0.8362995445901311, 0.8357897107396658))\n",
      "For batch 1 and degree : 4 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0001, (0.2315874496225282, 0.24314710727620556, 0.8395118106295666, 0.8387548793914523))\n",
      "For batch 1 and degree : 5 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0001, (0.22934022558531775, 0.4487383610616935, 0.8414573115804225, 0.8408067260534481))\n",
      "For batch 1 and degree : 6 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.00012663801734674035, (0.228940696079086, 0.30013770720377964, 0.8424144229806827, 0.8417075367831048))\n",
      "For batch 1 and degree : 7 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0004124626382901352, (0.22680842055372247, 1.5701176846727192, 0.8437155940346313, 0.8428085276749074))\n",
      "For batch 2 and degree : 0 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.011253355826007646, (0.4093171171682862, 0.4115166484672308, 0.6710165720918235, 0.6715566159401599))\n",
      "For batch 2 and degree : 1 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.03665241237079626, (0.3454519283746167, 0.3477835707544359, 0.7531677198865101, 0.7525148310549394))\n",
      "For batch 2 and degree : 2 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.018047217668271703, (0.32551492162513096, 0.3242209464794471, 0.7722627031209698, 0.775406241939644))\n",
      "For batch 2 and degree : 3 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.046415888336127774, (0.3234196289772558, 0.3208286942151171, 0.7769054681454733, 0.7805003868970853))\n",
      "For batch 2 and degree : 4 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.003455107294592218, (0.314141507444895, 0.31159162095283766, 0.7841114263605881, 0.7890121227753417))\n",
      "For batch 2 and degree : 5 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.014251026703029978, (0.31292378476829597, 0.3107445186588185, 0.7866101367036369, 0.7903017797265928))\n",
      "For batch 2 and degree : 6 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.014251026703029978, (0.31152104103506056, 0.3090122229843221, 0.7865778952798556, 0.7924297136961568))\n",
      "For batch 2 and degree : 7 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.008886238162743407, (0.30483956393233613, 0.30271105268090254, 0.7927682486458602, 0.7959117874645344))\n",
      "For batch 3 and degree : 0 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.004375479375074184, (0.3670121913467768, 0.36873342915266627, 0.7338365039977943, 0.7366280672732286))\n",
      "For batch 3 and degree : 1 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0001603718743751331, (0.3080089240737262, 0.3101748113530609, 0.7892714364488558, 0.7921146953405018))\n",
      "For batch 3 and degree : 2 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0013433993325989001, (0.3032348205481079, 0.30504961322375007, 0.7937000275709953, 0.7952164323132065))\n",
      "For batch 3 and degree : 3 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0008376776400682916, (0.29092702864358067, 0.2948865505980593, 0.8084849738075545, 0.8091397849462365))\n",
      "For batch 3 and degree : 4 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.008886238162743407, (0.2853692286489129, 0.2873816334171238, 0.8157912875654811, 0.8174800110283982))\n",
      "For batch 3 and degree : 5 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0006614740641230146, (0.28201483986499093, 0.2845519060515579, 0.8176867934932451, 0.8204438930245382))\n",
      "For batch 3 and degree : 6 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0001, (0.2788235983646754, 0.28245641016166345, 0.8191342707471739, 0.8217535153019024))\n",
      "For batch 3 and degree : 7 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.00025719138090593444, (0.26835061634272156, 0.271651836448498, 0.8253205128205128, 0.8298180314309347))\n"
     ]
    }
   ],
   "source": [
    "(best_lambda1,loss_acc_1),(best_lambda2,loss_acc_2),(best_lambda3,loss_acc_3) = find_best_overall_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.0004124626382901352,\n",
       "  (0.22680842055372247,\n",
       "   1.5701176846727192,\n",
       "   0.8437155940346313,\n",
       "   0.8428085276749074)),\n",
       " (0.008886238162743407,\n",
       "  (0.30483956393233613,\n",
       "   0.30271105268090254,\n",
       "   0.7927682486458602,\n",
       "   0.7959117874645344)),\n",
       " (0.00025719138090593444,\n",
       "  (0.26835061634272156,\n",
       "   0.271651836448498,\n",
       "   0.8253205128205128,\n",
       "   0.8298180314309347)))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(best_lambda1,loss_acc_1),(best_lambda2,loss_acc_2),(best_lambda3,loss_acc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_overall_param2():\n",
    "    for deg in range(8,15):\n",
    "        best_lambda1,loss_acc_1 = find_best_lambda(y_1,tx_1,deg)\n",
    "        print(\"For batch \" + str(1) +\" and degree \" + str(deg) + \" The best Test (lambda,(losstr,losste,acctr,accte)) : \"+ str((best_lambda1,loss_acc_1)))\n",
    "    \n",
    "    for deg in range(8,15):\n",
    "        best_lambda2,loss_acc_2 = find_best_lambda(y_2,tx_2,deg)\n",
    "        print(\"For batch \" + str(2) +\" and degree \" + str(deg) + \" The best Test (lambda,(losstr,losste,acctr,accte)) : \"+ str((best_lambda2,loss_acc_2)))\n",
    "    \n",
    "    for deg in range(8,15):\n",
    "        best_lambda3,loss_acc_3 = find_best_lambda(y_3,tx_3,deg)\n",
    "        print(\"For batch \" + str(3) +\" and degree \" + str(deg) + \" The best Test (lambda,(losstr,losste,acctr,accte)) : \"+ str((best_lambda3,loss_acc_3)))\n",
    "        \n",
    "    return (best_lambda1,loss_acc_1),(best_lambda2,loss_acc_2),(best_lambda3,loss_acc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For batch 1 and degree 8 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0001603718743751331, (0.22460794609718232, 7.8884319405228185, 0.8457924632168952, 0.8455359823841457))\n",
      "For batch 1 and degree 9 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0001603718743751331, (0.2239903660605052, 0.8375191665376706, 0.8464367931138024, 0.84618656791112))\n",
      "For batch 1 and degree 10 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0001, (0.2236858067487893, 42.294082877087355, 0.8464868381543389, 0.8463367030327296))\n",
      "For batch 1 and degree 11 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.003455107294592218, (0.22677148719526288, 393516175822863.2, 0.8461959513562205, 0.845248223401061))\n",
      "For batch 1 and degree 12 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0001603718743751331, (0.22300894328367965, 18898.77006311145, 0.8474189270343309, 0.8467370633570213))\n",
      "For batch 1 and degree 13 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.00012663801734674035, (0.22443450951418667, 47437.187005466796, 0.8467308077269543, 0.8458112301070964))\n",
      "For batch 1 and degree 14 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0008376776400682916, (0.2257608568490982, 7.835845357609034e+19, 0.8461532045507623, 0.844977146098155))\n",
      "For batch 2 and degree 8 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0002030917620904735, (0.2948903309410026, 0.2924418395292841, 0.801924812999742, 0.8028114521537271))\n",
      "For batch 2 and degree 9 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0006614740641230146, (0.2908343325133293, 0.28749151508804227, 0.8049555068351818, 0.8061645602269796))\n",
      "For batch 2 and degree 10 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0013433993325989001, (0.2905617341897881, 0.2877751986420653, 0.8056164560226979, 0.8057776631416044))\n",
      "For batch 2 and degree 11 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.003455107294592218, (0.289541600095466, 0.28626244338261103, 0.8055036110394636, 0.8063580087696672))\n",
      "For batch 2 and degree 12 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0004124626382901352, (0.28524502189773177, 0.28338361914579796, 0.8078572349754966, 0.8075831828733557))\n",
      "For batch 2 and degree 13 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0001603718743751331, (0.2839261486560952, 0.29267382858090524, 0.8083408563322156, 0.8090018055197318))\n",
      "For batch 2 and degree 14 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0001603718743751331, (0.28465606962373907, 0.2983496910128201, 0.808502063451122, 0.808099045653856))\n",
      "For batch 3 and degree 8 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.00025719138090593444, (0.25676983139223225, 0.26112659175686237, 0.8350909842845327, 0.8376068376068376))\n",
      "For batch 3 and degree 9 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.00012663801734674035, (0.2519759347649514, 0.2564809024434003, 0.8386752136752137, 0.8412599944858009))\n",
      "For batch 3 and degree 10 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0004124626382901352, (0.25178127559571073, 0.335508513375897, 0.8386235180590019, 0.8407775020678246))\n",
      "For batch 3 and degree 11 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.00012663801734674035, (0.24997019198710618, 0.33243019980662303, 0.8399503722084367, 0.8422249793217536))\n",
      "For batch 3 and degree 12 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0008376776400682916, (0.24592848812007712, 1.1216834050028173, 0.8422594430658947, 0.8447063688999172))\n",
      "For batch 3 and degree 13 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.002154434690031882, (0.24476609720368717, 4.589155691900101, 0.8428453267162944, 0.8451199338296113))\n",
      "For batch 3 and degree 14 The best Test (lambda,(losstr,losste,acctr,accte)) : (0.0010608183551394483, (0.24481631990115676, 1382.134407950883, 0.8436379928315412, 0.8427074717397298))\n"
     ]
    }
   ],
   "source": [
    "(best_lambda4,loss_acc_4),(best_lambda5,loss_acc_5),(best_lambda6,loss_acc_6) = find_best_overall_param2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result for ridges regression\n",
    "#1 lambda = 0.0001                 degree = 10  mean acc = 0.8467370633570213   std acc = 0.0007417655852458556 \n",
    "#2 lambda = 0.0004124626382901352  degree = 12  mean acc = 0.808099045653856    std acc = 0.00018879961116990187\n",
    "#3 lambda = 0.0008376776400682916  degree = 12  mean acc = 0.8451199338296113   std acc = 0.000357086967451429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for submission :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_1= expand_with_pairwise_products(tx_1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_2= expand_with_pairwise_products(tx_2,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_3= expand_with_pairwise_products(tx_3,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 , l1 = imp.ridge_regression(y_1,tx_1, 0.0001603718743751331)\n",
    "w2 , l2 = imp.ridge_regression(y_2,tx_2,0.0001603718743751331)\n",
    "w3 , l3 = imp.ridge_regression(y_3,tx_3,0.002154434690031882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_1 = expand_with_pairwise_products(xtest_1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_2 = expand_with_pairwise_products(xtest_2,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_3 = expand_with_pairwise_products(xtest_3,13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best parameters with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_lg(y, x, degree):\n",
    "    gamma = 0.001\n",
    "    max_iter = 2000\n",
    "    k_fold = 5 \n",
    "    seed = 12\n",
    "    initial_w = np.zeros((x.shape[1], 1))\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    loss_te = np.array([])\n",
    "    loss_tr = np.array([])\n",
    "    acc_te = np.array([]) \n",
    "    acc_tr = np.array([]) \n",
    "    #print(x.shape)\n",
    "    for i in range(k_fold):\n",
    "        sub = np.delete(k_indices,i,axis=0).flatten()\n",
    "        x_train = x[sub]\n",
    "        y_train = y[sub]\n",
    "        x_test = x[k_indices[i]]\n",
    "        y_test = y[k_indices[i]]\n",
    "\n",
    "        #print('train expanded',x_train.shape)\n",
    "        #print('test expanded',x_test.shape)\n",
    "        \n",
    "        x_train = expand_with_pairwise_products(x_train,degree)\n",
    "        x_test = expand_with_pairwise_products(x_test,degree)\n",
    "        #x_train = np.append(x_train,np.exp(x_train),axis=1)\n",
    "        \n",
    "        #x_train = np.append(x_train,np.cos(x_train),axis=1)\n",
    "        #x_train = np.append(x_train,np.sin(x_train),axis=1)\n",
    "        #x_train = add_bias(x_train)\n",
    "        #x_test = np.append(x_test,np.exp(x_test),axis=1)\n",
    "        #x_test = np.append(x_test,np.cos(x_test),axis=1)\n",
    "        #x_test = np.append(x_test,np.sin(x_test),axis=1)\n",
    "        \n",
    "        #x_test = add_bias(x_test)\n",
    "\n",
    "\n",
    "        #w,loss_train = imp.least_squares(y_train, x_train)\n",
    "        w,loss_train = imp.logistic_regression(y_train, x_train,initial_w,max_iter,gamma) \n",
    "        acc_train = compute_accuracy(y_train,x_train,w)\n",
    "        acc_test = compute_accuracy(y_test,x_test,w)\n",
    "        #print(loss_train)\n",
    "        loss_test = imp.calculate_loss(w, x_test, y_test)\n",
    "        loss_tr = np.append(loss_tr,loss_train)\n",
    "        loss_te= np.append(loss_te,loss_test)\n",
    "        acc_tr = np.append(acc_tr,acc_train)\n",
    "        acc_te = np.append(acc_te,acc_test)\n",
    "    \n",
    "    return degree, (np.mean(loss_tr),np.mean(loss_te),np.mean(acc_tr), np.mean(acc_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_degree():\n",
    "    for deg in range(8):\n",
    "        degree1,loss_acc_1 = cross_validation_lg(y_1,tx_1, deg)\n",
    "        print(\"For batch \" + str(1) +\" and degree : \" + str(deg) + \" The best Test (lambda,(losstr,losste,acctr,accte)) : \"+ str((best_lambda1,loss_acc_1)))\n",
    "    \n",
    "    for deg in range(8):\n",
    "        degree2,loss_acc_2 = cross_validation_lg(y_2,tx_2,deg)\n",
    "        print(\"For batch \" + str(2) +\" and degree : \" + str(deg) + \" The best Test (lambda,(losstr,losste,acctr,accte)) : \"+ str((best_lambda2,loss_acc_2)))\n",
    "    \n",
    "    for deg in range(8):\n",
    "        degree3,loss_acc_2 = cross_validation_lg(y_3,tx_3,deg)\n",
    "        print(\"For batch \" + str(3) +\" and degree : \" + str(deg) + \" The best Test (lambda,(losstr,losste,acctr,accte)) : \"+ str((best_lambda3,loss_acc_3)))\n",
    "        \n",
    "    return (degree1,loss_acc_1),(degree2,loss_acc_2),(degree3,loss_acc_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (79928,121) and (16,1) not aligned: 121 (dim 1) != 16 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8a6d295b7906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mdegree1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_acc_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_acc_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_acc_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_degree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-c1a5a69abdf3>\u001b[0m in \u001b[0;36mfind_best_degree\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_best_degree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdeg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdegree1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_acc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_lg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"For batch \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\" and degree : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" The best Test (lambda,(losstr,losste,acctr,accte)) : \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_lambda1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_acc_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-d89a920b919c>\u001b[0m in \u001b[0;36mcross_validation_lg\u001b[0;34m(y, x, degree)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#w,loss_train = imp.least_squares(y_train, x_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ML/scripts/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m# start the logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mgradient\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ML/scripts/implementations.py\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(x, A, b)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogsig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (79928,121) and (16,1) not aligned: 121 (dim 1) != 16 (dim 0)"
     ]
    }
   ],
   "source": [
    "(degree1,loss_acc_1),(degree2,loss_acc_2),(degree3,loss_acc_2) = find_best_degree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = predict_labels(w1, xtest_1)\n",
    "y_pred2 = predict_labels(w2, xtest_2)\n",
    "y_pred3 = predict_labels(w3, xtest_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate((y_pred1,y_pred2))\n",
    "y = np.concatenate((y,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.concatenate((ids_1,ids_2))\n",
    "ids = np.concatenate((ids,ids_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-9bab8864518e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../resources/gd.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/ML/scripts/proj1_helpers.py\u001b[0m in \u001b[0;36mcreate_csv_submission\u001b[0;34m(ids, y_pred, name)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Prediction'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../resources/gd.csv'\n",
    "create_csv_submission(ids, y,OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
