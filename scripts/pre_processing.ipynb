{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import math\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTrainingDataset():\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        my_dict = {'s': 1, 'b': -1}\n",
    "        self.y = np.loadtxt(path, delimiter=\",\",dtype=np.str_,skiprows=1,usecols=1) \n",
    "        self.y = np.array([my_dict[i] for i in self.y])\n",
    "        self.y = self.y.reshape(self.y.shape[0],1)\n",
    "        self.x = np.loadtxt(path, delimiter=\",\",skiprows=1,usecols=range(2,32))\n",
    "        self.ids = np.loadtxt(path, delimiter=\",\",skiprows=1,usecols=0)\n",
    "        self.ids = self.ids.reshape(self.ids.shape[0],1)\n",
    "        self.n_samples = self.x.shape[0]\n",
    "            \n",
    "    def __getitem__(self,i):\n",
    "        return self.x[i] , self.y[i] , self.ids[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.x , self.y ,self.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTestingDataset():\n",
    "    def __init__(self,path):\n",
    "        self.x = np.loadtxt(path, delimiter=\",\",skiprows=1,usecols=range(2,32))\n",
    "        self.ids = np.loadtxt(path, delimiter=\",\",skiprows=1,usecols=0)\n",
    "        self.ids = self.ids.reshape(self.ids.shape[0],1)\n",
    "        #Standardize Data Manually for each separate group of data after Distribution manipulation\n",
    "        #self.x = (self.x - np.mean(self.x, axis=0))/np.std(self.x, axis=0)\n",
    "        self.n_samples = self.x.shape[0]\n",
    "            \n",
    "    def __getitem__(self,i):\n",
    "        return self.x[i] , self.ids[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.x , self.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(tx,y,ids):\n",
    "    \n",
    "    array_1 = np.where( (tx[:,22]!=0))[0]\n",
    "    tx_1 = np.delete(tx, array_1,0)\n",
    "    y_1 = np.delete(y, array_1,0)\n",
    "    ids_1 = np.delete(ids, array_1,0)\n",
    "    \n",
    "    array_2 = np.where( (tx[:,22]!=1))[0]\n",
    "    tx_2 = np.delete(tx, array_2,0)\n",
    "    y_2 = np.delete(y, array_2,0)\n",
    "    ids_2 = np.delete(ids, array_2,0)\n",
    "    \n",
    "    array_3 = np.nonzero(np.logical_or((tx[:,22]==0), (tx[:,22]==1)))[0]\n",
    "    tx_3 = np.delete(tx, array_3, 0)\n",
    "    y_3 = np.delete(y, array_3, 0)\n",
    "    ids_3 = np.delete(ids, array_3, 0)\n",
    "    \n",
    "    return (tx_1,y_1,ids_1) , (tx_2,y_2,ids_2) , (tx_3,y_3,ids_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_features(data_n):\n",
    "    \n",
    "    tx_n = data_n[0]\n",
    "    to_delete = np.zeros(tx_n.shape[0])-999\n",
    "    indexes = [] \n",
    "    for i in range(tx_n.shape[1]):\n",
    "        #if(sum(tx_n[:,i]-to_delete)==0 or sum(tx_n[:,i])==0 ):\n",
    "            #indexes.append(i)\n",
    "        # Find unique values in column along with their length \n",
    "        # if len is == 1 then it contains same values i.e -999 so features to drop or zeros\n",
    "        if len(np.unique(tx_n[:,i])) == 1:  \n",
    "            indexes.append(i)\n",
    "    \n",
    "    tx_n =np.delete(tx_n,indexes,1)\n",
    "    \n",
    "    return (tx_n,data_n[1],data_n[2]),indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_median(data_n):\n",
    "    \n",
    "    tx_n = data_n[0]\n",
    "    for i in range(tx_n.shape[1]):\n",
    "        column = tx_n[:,i]\n",
    "        m = np.median(column[column!=-999])\n",
    "        column[column == -999] = m\n",
    "        tx_n[:,i]=column\n",
    "        \n",
    "    return (tx_n,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction_corr(data_n):\n",
    "    tx_n = data_n[0]\n",
    "    corr = np.corrcoef(tx_n.T)\n",
    "    pairs = np.argwhere(np.triu(np.isclose(corr,1,rtol=2e-01),1))\n",
    "    indexes = pairs[:,1]\n",
    "    tx_n = np.delete(tx_n,indexes,axis=1)\n",
    "    return (tx_n,data_n[1],data_n[2]),indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction_PCA(data_n,k):\n",
    "    # Compare performance with dimen reduc correlation first\n",
    "    tx_n = data_n[0]\n",
    "    #standardize the data \n",
    "    tx_n = (tx_n - np.mean(tx_n, axis=0))/np.std(tx_n, axis=0)\n",
    "    #compute covariance matrix \n",
    "    cov = np.cov(tx_n.T)\n",
    "    #find eighenvalues and eighenvector\n",
    "    ei_val, ei_vect = np.linalg.eig(cov)\n",
    "    # Make list of tuples and sort in decreasing eighenvalue order \n",
    "    ei_tuple = [(np.abs(ei_val[i]), ei_vect[:, i]) for i in range(len(ei_val))]\n",
    "    ei_tuple.sort(key = lambda k: k[0], reverse = True)\n",
    "    #Keep the eighenvectors corresponding to the K largest eighenvalues (TRADE-OFF TO BE MADE)\n",
    "    ei_tuple_largest = ei_tuple[:k]\n",
    "    #Construct a projection matrix W from the these eigenvectors\n",
    "    w = np.stack([vec for val, vec in ei_tuple_largest], axis=1)\n",
    "    #Transform X using the projection matrix W to obtain the new k-dimensional feature subspace\n",
    "    #TODO \n",
    "    return ei_tuple, ei_tuple_largest,w\n",
    "    #return data_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_transform(data_n):\n",
    "    tx_n = data_n[0]\n",
    "    # Scaling Data to apply Log function\n",
    "    dividor = np.max(tx_n,axis=0)-np.min(tx_n,axis=0)   \n",
    "    tx_n = (tx_n - np.min(tx_n,axis=0))/dividor\n",
    "    # Transforming Data\n",
    "    tx_n = np.log(1+tx_n)\n",
    "    # Normalizing Data\n",
    "    centered_data = tx_n - np.mean(tx_n, axis=0)\n",
    "    std_data = centered_data / np.std(centered_data, axis=0)\n",
    "    return (std_data,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put it all together \n",
    "def process_data(path) : \n",
    "    #load the initial training set \n",
    "    train_data = LoadTrainingDataset(path)\n",
    "    #extract features, predictions and ids\n",
    "    tx , y ,ids = train_data.get_data()\n",
    "    #separate data into 3 categories according to the PRI_JEST_NUM \n",
    "    data_1,data_2,data_3 = separate_data(tx,y,ids)\n",
    "    #remove column that contain only 1 data value (-999, 0 or 1 in our case)\n",
    "    data_1,indexes1 = adapt_features(data_1)\n",
    "    data_2,indexes2 = adapt_features(data_2)\n",
    "    data_3,indexes3 = adapt_features(data_3)\n",
    "    #replace -999 coefficients with the mediane of the column (-999 coefficients discared)\n",
    "    data_1 = add_median(data_1)\n",
    "    data_2 = add_median(data_2)\n",
    "    data_3 = add_median(data_3)\n",
    "    #dimentionality reduction : remove column that are strongly correlated\n",
    "    data_1 , indexes4 = dimensionality_reduction_corr(data_1)\n",
    "    data_2 , indexes5 = dimensionality_reduction_corr(data_2)\n",
    "    data_3 , indexes6 = dimensionality_reduction_corr(data_3)\n",
    "    #\n",
    "    data_1 = scale_transform(data_1)\n",
    "    data_2 = scale_transform(data_2)\n",
    "    data_3 = scale_transform(data_3)\n",
    "    return (data_1[0],data_1[1],data_1[2]), (data_2[0],data_2[1],data_2[2]), (data_3[0],data_3[1],data_3[2]),indexes1,indexes2,indexes3,indexes4,indexes5,indexes6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test(path,indexes1,indexes2,indexes3,indexes4,indexes5,indexes6):\n",
    "    #load the initial test set \n",
    "    test_data = LoadTestingDataset(path)\n",
    "    #extract features and ids\n",
    "    tX_test, ids_test = test_data.get_data()  \n",
    "    #separate data into 3 categories according to the PRI_JEST_NUM \n",
    "    y = np.ones((tX_test.shape[0],1))\n",
    "    data_1,data_2,data_3 = separate_data(tX_test,y,ids_test)\n",
    "    #remove column that have been dropped during test phase\n",
    "    xtest1 = data_1[0] \n",
    "    xtest1 = np.delete(xtest1,indexes1,1)\n",
    "    xtest2 = data_2[0] \n",
    "    xtest2 = np.delete(xtest2,indexes2,1)\n",
    "    xtest3 = data_3[0]\n",
    "    xtest3 = np.delete(xtest3,indexes3,1)\n",
    "    data_1 = (xtest1,data_1[1],data_1[2])\n",
    "    data_2 = (xtest2,data_2[1],data_2[2])\n",
    "    data_3 = (xtest3,data_3[1],data_3[2])\n",
    "    data_1 = add_median(data_1)\n",
    "    data_2 = add_median(data_2)\n",
    "    data_3 = add_median(data_3)  \n",
    "    #dimentionality reduction : remove column that are strongly correlated\n",
    "    xtest1 = data_1[0] \n",
    "    xtest1 = np.delete(xtest1,indexes4,1)\n",
    "    xtest2 = data_2[0] \n",
    "    xtest2 = np.delete(xtest2,indexes5,1)\n",
    "    xtest3 = data_3[0]\n",
    "    xtest3 = np.delete(xtest3,indexes6,1)\n",
    "    data_1 = (xtest1,data_1[1],data_1[2])\n",
    "    data_2 = (xtest2,data_2[1],data_2[2])\n",
    "    data_3 = (xtest3,data_3[1],data_3[2])\n",
    "    #Scaling and transforming data\n",
    "    data_1 = scale_transform(data_1)\n",
    "    data_2 = scale_transform(data_2)\n",
    "    data_3 = scale_transform(data_3)\n",
    "    return (data_1[0],data_1[2]), (data_2[0],data_2[2]), (data_3[0],data_3[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
