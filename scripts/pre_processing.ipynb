{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset():\n",
    "    \n",
    "    def __init__(self):\n",
    "        my_dict = {'s': 1, 'b': -1}\n",
    "        self.y = np.loadtxt('train.csv', delimiter=\",\",dtype=np.str_,skiprows=1,usecols=1) \n",
    "        self.y = np.array([my_dict[i] for i in self.y])\n",
    "        self.y = self.y.reshape(self.y.shape[0],1)\n",
    "        self.x = np.loadtxt('train.csv', delimiter=\",\",skiprows=1,usecols=range(2,32))\n",
    "        self.ids = np.loadtxt('train.csv', delimiter=\",\",skiprows=1,usecols=0)\n",
    "        self.ids = self.ids.reshape(self.ids.shape[0],1)\n",
    "        #Standardize Data Manually for each separate group of data after Distribution manipulation\n",
    "        #self.x = (self.x - np.mean(self.x, axis=0))/np.std(self.x, axis=0)\n",
    "        self.n_samples = self.x.shape[0]\n",
    "            \n",
    "    def __getitem__(self,i):\n",
    "        return self.x[i] , self.y[i] , self.ids[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.x , self.y ,self.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = LoadDataset()\n",
    "tx , y ,ids = train_data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(tx,y,ids):\n",
    "    \n",
    "    array_1 = np.where( (tx[:,22]!=0))[0]\n",
    "    tx_1 = np.delete(tx, array_1,0)\n",
    "    y_1 = np.delete(y, array_1,0)\n",
    "    ids_1 = np.delete(ids, array_1,0)\n",
    "    \n",
    "    array_2 = np.where( (tx[:,22]!=1))[0]\n",
    "    tx_2 = np.delete(tx, array_2,0)\n",
    "    y_2 = np.delete(y, array_2,0)\n",
    "    ids_2 = np.delete(ids, array_2,0)\n",
    "    \n",
    "    array_3 = np.nonzero(np.logical_or((tx[:,22]==0), (tx[:,22]==1)))[0]\n",
    "    tx_3 = np.delete(tx, array_3, 0)\n",
    "    y_3 = np.delete(y, array_3, 0)\n",
    "    ids_3 = np.delete(ids, array_3, 0)\n",
    "    \n",
    "    return (tx_1,y_1,ids_1) , (tx_2,y_2,ids_2) , (tx_3,y_3,ids_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_features(data_n):\n",
    "    \n",
    "    tx_n = data_n[0]\n",
    "    to_delete = np.zeros(tx_n.shape[0])-999\n",
    "    indexes = [] \n",
    "    for i in range(tx_n.shape[1]):\n",
    "        #if(sum(tx_n[:,i]-to_delete)==0 or sum(tx_n[:,i])==0 ):\n",
    "            #indexes.append(i)\n",
    "        # Find unique values in column along with their length \n",
    "        # if len is == 1 then it contains same values i.e -999 so features to drop or zeros\n",
    "        if len(np.unique(tx_n[:,i])) == 1:  \n",
    "            indexes.append(i)\n",
    "    \n",
    "    tx_n =np.delete(tx_n,indexes,1)\n",
    "    \n",
    "    return (tx_n,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1,data_2,data_3 = separate_data(tx,y,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = adapt_features(data_1)\n",
    "data_2 = adapt_features(data_2)\n",
    "data_3 = adapt_features(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_median(data_n):\n",
    "    \n",
    "    tx_n = data_n[0]\n",
    "    for i in range(tx_n.shape[1]):\n",
    "        column = tx_n[:,i]\n",
    "        m = np.median(column[column!=-999])\n",
    "        column[column == -999] = m\n",
    "        tx_n[:,i]=column\n",
    "        \n",
    "    return (tx_n,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = add_median(data_1)\n",
    "data_2 = add_median(data_2)\n",
    "data_3 = add_median(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction_corr(data_n):\n",
    "    tx_n = data_n[0]\n",
    "    corr = np.corrcoef(tx_n.T)\n",
    "    pairs = np.argwhere(np.triu(np.isclose(corr,1,rtol=2e-01),1))\n",
    "    tx_n = np.delete(tx_n,pairs[:,1],axis=1)\n",
    "    return (tx_n,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1=dimensionality_reduction_corr(data_1)\n",
    "data_2=dimensionality_reduction_corr(data_2)\n",
    "data_3=dimensionality_reduction_corr(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction_PCA(data_n):\n",
    "    # Compare performance with dimen reduc correlation first\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_transform(data_n):\n",
    "    tx_n = data_n[0]\n",
    "    # Scaling Data to apply Log function\n",
    "    dividor = np.max(tx_n,axis=0)-np.min(tx_n,axis=0)   \n",
    "    tx_n = (tx_n - np.min(tx_n,axis=0))/dividor\n",
    "    # Transforming Data\n",
    "    tx_n = np.log(1+tx_n)\n",
    "    # Normalizing Data\n",
    "    centered_data = tx_n - np.mean(tx_n, axis=0)\n",
    "    std_data = centered_data / np.std(centered_data, axis=0)\n",
    "    return (std_data,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = scale_transform(data_1)\n",
    "data_2 = scale_transform(data_2)\n",
    "data_3 = scale_transform(data_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
