{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset():\n",
    "    \n",
    "    def __init__(self):\n",
    "        my_dict = {'s': 1, 'b': -1}\n",
    "        self.y = np.loadtxt('../resources/train.csv', delimiter=\",\",dtype=np.str_,skiprows=1,usecols=1) \n",
    "        self.y = np.array([my_dict[i] for i in self.y])\n",
    "        self.y = self.y.reshape(self.y.shape[0],1)\n",
    "        self.x = np.loadtxt('../resources/train.csv', delimiter=\",\",skiprows=1,usecols=range(2,32))\n",
    "        self.ids = np.loadtxt('../resources/train.csv', delimiter=\",\",skiprows=1,usecols=0)\n",
    "        self.ids = self.ids.reshape(self.ids.shape[0],1)\n",
    "        #Standardize Data Manually for each separate group of data after Distribution manipulation\n",
    "        #self.x = (self.x - np.mean(self.x, axis=0))/np.std(self.x, axis=0)\n",
    "        self.n_samples = self.x.shape[0]\n",
    "            \n",
    "    def __getitem__(self,i):\n",
    "        return self.x[i] , self.y[i] , self.ids[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.x , self.y ,self.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = LoadDataset()\n",
    "tx , y ,ids = train_data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(tx,y,ids):\n",
    "    \n",
    "    array_1 = np.where( (tx[:,22]!=0))[0]\n",
    "    tx_1 = np.delete(tx, array_1,0)\n",
    "    y_1 = np.delete(y, array_1,0)\n",
    "    ids_1 = np.delete(ids, array_1,0)\n",
    "    \n",
    "    array_2 = np.where( (tx[:,22]!=1))[0]\n",
    "    tx_2 = np.delete(tx, array_2,0)\n",
    "    y_2 = np.delete(y, array_2,0)\n",
    "    ids_2 = np.delete(ids, array_2,0)\n",
    "    \n",
    "    array_3 = np.nonzero(np.logical_or((tx[:,22]==0), (tx[:,22]==1)))[0]\n",
    "    tx_3 = np.delete(tx, array_3, 0)\n",
    "    y_3 = np.delete(y, array_3, 0)\n",
    "    ids_3 = np.delete(ids, array_3, 0)\n",
    "    \n",
    "    return (tx_1,y_1,ids_1) , (tx_2,y_2,ids_2) , (tx_3,y_3,ids_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_features(data_n):\n",
    "    \n",
    "    tx_n = data_n[0]\n",
    "    to_delete = np.zeros(tx_n.shape[0])-999\n",
    "    indexes = [] \n",
    "    for i in range(tx_n.shape[1]):\n",
    "        #if(sum(tx_n[:,i]-to_delete)==0 or sum(tx_n[:,i])==0 ):\n",
    "            #indexes.append(i)\n",
    "        # Find unique values in column along with their length \n",
    "        # if len is == 1 then it contains same values i.e -999 so features to drop or zeros\n",
    "        if len(np.unique(tx_n[:,i])) == 1:  \n",
    "            indexes.append(i)\n",
    "    \n",
    "    tx_n =np.delete(tx_n,indexes,1)\n",
    "    \n",
    "    return (tx_n,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1,data_2,data_3 = separate_data(tx,y,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = adapt_features(data_1)\n",
    "data_2 = adapt_features(data_2)\n",
    "data_3 = adapt_features(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_median(data_n):\n",
    "    \n",
    "    tx_n = data_n[0]\n",
    "    for i in range(tx_n.shape[1]):\n",
    "        column = tx_n[:,i]\n",
    "        m = np.median(column[column!=-999])\n",
    "        column[column == -999] = m\n",
    "        tx_n[:,i]=column\n",
    "        \n",
    "    return (tx_n,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = add_median(data_1)\n",
    "data_2 = add_median(data_2)\n",
    "data_3 = add_median(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction_corr(data_n):\n",
    "    tx_n = data_n[0]\n",
    "    corr = np.corrcoef(tx_n.T)\n",
    "    pairs = np.argwhere(np.triu(np.isclose(corr,1,rtol=2e-01),1))\n",
    "    tx_n = np.delete(tx_n,pairs[:,1],axis=1)\n",
    "    return (tx_n,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1=dimensionality_reduction_corr(data_1)\n",
    "data_2=dimensionality_reduction_corr(data_2)\n",
    "data_3=dimensionality_reduction_corr(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction_PCA(data_n,k):\n",
    "    # Compare performance with dimen reduc correlation first\n",
    "    tx_n = data_n[0]\n",
    "    #standardize the data \n",
    "    tx_n = (tx_n - np.mean(tx_n, axis=0))/np.std(tx_n, axis=0)\n",
    "    #compute covariance matrix \n",
    "    cov = np.cov(tx_n.T)\n",
    "    #find eighenvalues and eighenvector\n",
    "    ei_val, ei_vect = np.linalg.eig(cov)\n",
    "    # Make list of tuples and sort in decreasing eighenvalue order \n",
    "    ei_tuple = [(np.abs(ei_val[i]), ei_vect[:, i]) for i in range(len(ei_val))]\n",
    "    ei_tuple.sort(key = lambda k: k[0], reverse = True)\n",
    "    #Keep the eighenvectors corresponding to the K largest eighenvalues (TRADE-OFF TO BE MADE)\n",
    "    ei_tuple_largest = ei_tuple[:k]\n",
    "    #Construct a projection matrix W from the these eigenvectors\n",
    "    w = np.stack([vec for val, vec in ei_tuple_largest], axis=1)\n",
    "    #Transform X using the projection matrix W to obtain the new k-dimensional feature subspace\n",
    "    #TODO \n",
    "    return ei_tuple, ei_tuple_largest,w\n",
    "    #return data_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "ei_tuple,ei_tuple_largest,w = dimensionality_reduction_PCA(data_1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "[x[1] for x in ei_tuple_largest][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "data_1[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 5)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_transform(data_n):\n",
    "    tx_n = data_n[0]\n",
    "    # Scaling Data to apply Log function\n",
    "    dividor = np.max(tx_n,axis=0)-np.min(tx_n,axis=0)   \n",
    "    tx_n = (tx_n - np.min(tx_n,axis=0))/dividor\n",
    "    # Transforming Data\n",
    "    tx_n = np.log(1+tx_n)\n",
    "    # Normalizing Data\n",
    "    centered_data = tx_n - np.mean(tx_n, axis=0)\n",
    "    std_data = centered_data / np.std(centered_data, axis=0)\n",
    "    return (std_data,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = scale_transform(data_1)\n",
    "data_2 = scale_transform(data_2)\n",
    "data_3 = scale_transform(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put it all together \n",
    "def process_data() : \n",
    "    #load the initial training set \n",
    "    train_data = LoadDataset()\n",
    "    #extract features, predictions and ids\n",
    "    tx , y ,ids = train_data.get_data()\n",
    "    #separate data into 3 categories according to the PRI_JEST_NUM \n",
    "    data_1,data_2,data_3 = separate_data(tx,y,ids)\n",
    "    #remove column that contain only 1 data value (-999, 0 or 1 in our case)\n",
    "    data_1 = adapt_features(data_1)\n",
    "    data_2 = adapt_features(data_2)\n",
    "    data_3 = adapt_features(data_3)\n",
    "    #replace -999 coefficients with the mediane of the column (-999 coefficients discared)\n",
    "    data_1 = add_median(data_1)\n",
    "    data_2 = add_median(data_2)\n",
    "    data_3 = add_median(data_3)\n",
    "    #dimentionality reduction : remove column that are strongly correlated\n",
    "    data_1=dimensionality_reduction_corr(data_1)\n",
    "    data_2=dimensionality_reduction_corr(data_2)\n",
    "    data_3=dimensionality_reduction_corr(data_3)\n",
    "    #TODO : add scaling - features explansion - log transformation \n",
    "    return (data_1[0],data_1[1],data_1[2]), (data_2[0],data_2[1],data_2[2]), (data_3[0],data_3[1],data_3[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
