{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import math\n",
    "from proj1_helpers import *\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTrainingDataset():\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        #we keep 0 & 1 if we use Logistic Regression otherwise we can take 1 ; -1 to\n",
    "        #follow the professor predict_label() method convention\n",
    "        my_dict = {'s': 1, 'b': 0}\n",
    "        self.y = np.loadtxt(path, delimiter=\",\",dtype=np.str_,skiprows=1,usecols=1) \n",
    "        self.y = np.array([my_dict[i] for i in self.y])\n",
    "        self.y = self.y.reshape(self.y.shape[0],1)\n",
    "        self.x = np.loadtxt(path, delimiter=\",\",skiprows=1,usecols=range(2,32))\n",
    "        self.ids = np.loadtxt(path, delimiter=\",\",skiprows=1,usecols=0)\n",
    "        self.ids = self.ids.reshape(self.ids.shape[0],1)\n",
    "        self.n_samples = self.x.shape[0]\n",
    "            \n",
    "    def __getitem__(self,i):\n",
    "        return self.x[i] , self.y[i] , self.ids[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.x , self.y ,self.ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTestingDataset():\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        self.x = np.loadtxt(path, delimiter=\",\",skiprows=1,usecols=range(2,32))\n",
    "        self.ids = np.loadtxt(path, delimiter=\",\",skiprows=1,usecols=0)\n",
    "        self.ids = self.ids.reshape(self.ids.shape[0],1)\n",
    "        self.n_samples = self.x.shape[0]\n",
    "            \n",
    "    def __getitem__(self,i):\n",
    "        return self.x[i] , self.ids[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.x , self.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(tx,y,ids):\n",
    "    \n",
    "    array_1 = np.where( (tx[:,22]!=0))[0]\n",
    "    tx_1 = np.delete(tx, array_1,0)\n",
    "    y_1 = np.delete(y, array_1,0)\n",
    "    ids_1 = np.delete(ids, array_1,0)\n",
    "    \n",
    "    array_2 = np.where( (tx[:,22]!=1))[0]\n",
    "    tx_2 = np.delete(tx, array_2,0)\n",
    "    y_2 = np.delete(y, array_2,0)\n",
    "    ids_2 = np.delete(ids, array_2,0)\n",
    "    \n",
    "    array_3 = np.nonzero(np.logical_or((tx[:,22]==0), (tx[:,22]==1)))[0]\n",
    "    tx_3 = np.delete(tx, array_3, 0)\n",
    "    y_3 = np.delete(y, array_3, 0)\n",
    "    ids_3 = np.delete(ids, array_3, 0)\n",
    "    \n",
    "    return (tx_1,y_1,ids_1) , (tx_2,y_2,ids_2) , (tx_3,y_3,ids_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_features(data_n):\n",
    "    \n",
    "    tx_n = data_n[0]\n",
    "    to_delete = np.zeros(tx_n.shape[0])-999\n",
    "    indexes = [] \n",
    "    for i in range(tx_n.shape[1]):\n",
    "        if len(np.unique(tx_n[:,i])) == 1:  \n",
    "            indexes.append(i)   \n",
    "    tx_n =np.delete(tx_n,indexes,1)\n",
    "    \n",
    "    return (tx_n,data_n[1],data_n[2]),indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_median(data_n):\n",
    "    \n",
    "    tx_n = data_n[0]\n",
    "    for i in range(tx_n.shape[1]):\n",
    "        column = tx_n[:,i]\n",
    "        m = np.median(column[column!=-999])\n",
    "        column[column == -999] = m\n",
    "        tx_n[:,i]=column\n",
    "        \n",
    "    return (tx_n,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction_corr(data_n):\n",
    "    \n",
    "    tx_n = data_n[0]\n",
    "    corr = np.corrcoef(tx_n.T)\n",
    "    pairs = np.argwhere(np.triu(np.isclose(corr,1,rtol=2e-01),1))\n",
    "    indexes = pairs[:,1]\n",
    "    tx_n = np.delete(tx_n,indexes,axis=1)\n",
    "    \n",
    "    return (tx_n,data_n[1],data_n[2]),indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction_PCA(data_n,k):\n",
    "    \n",
    "    # Compare performance with dimen reduc correlation first\n",
    "    tx_n = data_n[0]\n",
    "    #standardize the data \n",
    "    tx_n = (tx_n - np.mean(tx_n, axis=0))/np.std(tx_n, axis=0)\n",
    "    #compute covariance matrix \n",
    "    cov = np.cov(tx_n.T)\n",
    "    #find eighenvalues and eighenvector\n",
    "    ei_val, ei_vect = np.linalg.eig(cov)\n",
    "    # Make list of tuples and sort in decreasing eighenvalue order \n",
    "    ei_tuple = [(np.abs(ei_val[i]), ei_vect[:, i]) for i in range(len(ei_val))]\n",
    "    ei_tuple.sort(key = lambda k: k[0], reverse = True)\n",
    "    #Keep the eighenvectors corresponding to the K largest eighenvalues (TRADE-OFF TO BE MADE)\n",
    "    ei_tuple_largest = ei_tuple[:k]\n",
    "    #Construct a projection matrix W from the these eigenvectors\n",
    "    w = np.stack([vec for val, vec in ei_tuple_largest], axis=1)\n",
    "\n",
    "    return (ei_tuple, ei_tuple_largest,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_without_pairwise_products(X,M) :\n",
    "    \n",
    "    ans = np.ones((X.shape[0],1))\n",
    "    for idx in range(1,M+1): \n",
    "        ans=np.hstack((ans, X**idx))\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_with_pairwise_products(X, M):\n",
    "    \n",
    "    without_pairwise_products = expand_without_pairwise_products(X,M)\n",
    "    # create the interactions between two variable\n",
    "    # X is (N, d), we first make it as (N, d, 1) and (N, 1, d), then compute the interaction\n",
    "    X_inter = np.expand_dims(X, axis=1)\n",
    "    X_inter_ = np.expand_dims(X, axis=2)\n",
    "    full_interactions = np.matmul(X_inter_, X_inter)\n",
    "    # np.triu_indices: Return the indices for the upper-triangle of a matrix\n",
    "    indices = np.triu_indices(full_interactions.shape[1], k=1)\n",
    "    interactions = np.zeros((X.shape[0], len(indices[0])))\n",
    "    for n in range(X.shape[0]):\n",
    "        interactions[n] = full_interactions[n][indices]\n",
    "        \n",
    "    return np.concatenate((without_pairwise_products, interactions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    \n",
    "    X =np.append(X, np.ones(shape=(X.shape[0],1)), axis=1)\n",
    "    \n",
    "    return (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_transform(data_n,skewed):\n",
    "    \n",
    "    tx_n = data_n[0]\n",
    "    # Scaling Data to apply Log function\n",
    "    dividor = np.max(tx_n,axis=0)-np.min(tx_n,axis=0)   \n",
    "    tx_n = (tx_n - np.min(tx_n,axis=0))/dividor\n",
    "    #Log transform skewed Data\n",
    "    tx_n[:,skewed]= np.log(tx_n[:,skewed]+1)\n",
    "    #Normalizing Data\n",
    "    mean = np.mean(tx_n, axis=0)\n",
    "    centered_data = tx_n - mean\n",
    "    std = np.std(centered_data, axis=0)\n",
    "    std_data = centered_data / std\n",
    "    data_n = (std_data,data_n[1],data_n[2])\n",
    "    parameters = (mean,std)\n",
    "    \n",
    "    return data_n,parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_transform_test(data_n,parameters):\n",
    "    \n",
    "    xtest_n = data_n[0]\n",
    "    dividor = np.max(xtest_n,axis=0)-np.min(xtest_n,axis=0)   \n",
    "    xtest_n = (xtest_n - np.min(xtest_n,axis=0))/dividor\n",
    "    xtest_n = np.log(1+xtest_n)\n",
    "    centered_data = tx_n - np.mean(tx_n, axis=0)      \n",
    "    std_data = centered_data / np.std(centered_data, axis=0)    \n",
    "    data_n = (std_data,data_n[1],data_n[2])\n",
    "    \n",
    "    return (std_data,data_n[1],data_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put it all together for training data\n",
    "def process_data(path) : \n",
    "    #load the initial training set \n",
    "    train_data = LoadTrainingDataset(path)\n",
    "    #extract features, predictions and ids\n",
    "    tx , y ,ids = train_data.get_data()\n",
    "    #separate data into 3 categories according to the PRI_JEST_NUM \n",
    "    data_1,data_2,data_3 = separate_data(tx,y,ids)\n",
    "    #remove column that contain only 1 data value (-999, 0 or 1 in our case)\n",
    "    data_1,indexes1 = adapt_features(data_1)\n",
    "    data_2,indexes2 = adapt_features(data_2)\n",
    "    data_3,indexes3 = adapt_features(data_3)\n",
    "    #replace -999 coefficients with the mediane of the column (-999 coefficients discared)\n",
    "    data_1 = add_median(data_1)\n",
    "    data_2 = add_median(data_2)\n",
    "    data_3 = add_median(data_3)\n",
    "    #dimentionality reduction : remove column that are strongly correlated (above 80%)\n",
    "    data_1 , indexes4 = dimensionality_reduction_corr(data_1)\n",
    "    data_2 , indexes5 = dimensionality_reduction_corr(data_2)\n",
    "    data_3 , indexes6 = dimensionality_reduction_corr(data_3) \n",
    "    \n",
    "    #UNCOMMENT to see data distribution to inspect which features are skewed\n",
    "    #for data in (data_1,data_2,data_3):\n",
    "        #fig=plt.figure(figsize=(40,100))\n",
    "        #for i in range(data[0].shape[1]):\n",
    "            #plt.subplot(data[0].shape[1],2,i+1)\n",
    "            #sns.distplot(data[0][:, i],ax=plt.gca(),hist=False)\n",
    "            #plt.subplots_adjust\n",
    "        #plt.show()\n",
    "        \n",
    "    #scaling and transforming data \n",
    "    skewed1 = [2,4,6,10,15]\n",
    "    skewed2 = [2,3,5,7,10,13,15]\n",
    "    skewed3 = [1,2,3,4,7,8,11,14,17,22]\n",
    "    data_1,parameters1 = scale_transform(data_1,skewed1)\n",
    "    data_2,parameters2 = scale_transform(data_2,skewed2)\n",
    "    data_3,parameters3 = scale_transform(data_3,skewed3)\n",
    "    tx_n = data_1[0]\n",
    "\n",
    "    #storing indexes of the columns to drop for the test data for each model\n",
    "    indexes = (indexes1,indexes2,indexes3,indexes4,indexes5,indexes6)\n",
    "    \n",
    "    #storing means and std of each training set model\n",
    "    parameters = (parameters1,parameters2,parameters3)\n",
    "    return data_1, data_2, data_3 , indexes , parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put it all together for testing data\n",
    "def process_test(path,indexes,parameters):\n",
    "    #load the initial test set \n",
    "    test_data = LoadTestingDataset(path)\n",
    "    #extract features and ids\n",
    "    tX_test, ids_test = test_data.get_data()  \n",
    "    #separate data into 3 categories according to the PRI_JEST_NUM \n",
    "    y = np.ones((tX_test.shape[0],1))\n",
    "    data_1,data_2,data_3 = separate_data(tX_test,y,ids_test)\n",
    "    #remove column that have been dropped during test phase\n",
    "    xtest1 = data_1[0] \n",
    "    xtest1 = np.delete(xtest1,indexes[0],1)\n",
    "    xtest2 = data_2[0] \n",
    "    xtest2 = np.delete(xtest2,indexes[1],1)\n",
    "    xtest3 = data_3[0]\n",
    "    xtest3 = np.delete(xtest3,indexes[2],1)\n",
    "    data_1 = (xtest1,data_1[1],data_1[2])\n",
    "    data_2 = (xtest2,data_2[1],data_2[2])\n",
    "    data_3 = (xtest3,data_3[1],data_3[2])\n",
    "    data_1 = add_median(data_1)\n",
    "    data_2 = add_median(data_2)\n",
    "    data_3 = add_median(data_3)  \n",
    "    #dimentionality reduction : remove column that are strongly correlated (according to the training set analysis)\n",
    "    xtest1 = data_1[0] \n",
    "    xtest1 = np.delete(xtest1,indexes[3],1)\n",
    "    xtest2 = data_2[0] \n",
    "    xtest2 = np.delete(xtest2,indexes[4],1)\n",
    "    xtest3 = data_3[0]\n",
    "    xtest3 = np.delete(xtest3,indexes[5],1)\n",
    "    data_1 = (xtest1,data_1[1],data_1[2])\n",
    "    data_2 = (xtest2,data_2[1],data_2[2])\n",
    "    data_3 = (xtest3,data_3[1],data_3[2])\n",
    "    \n",
    "\n",
    "    #Log Transform skewed Data, scaling and normalizing using the same parameters of the training set\n",
    "    skewed1 = [2,4,6,10,15]\n",
    "    skewed2 = [2,3,5,7,10,13,15]\n",
    "    skewed3 = [1,2,3,4,7,8,11,14,17,22]\n",
    "    data_1,parameters1 = scale_transform(data_1,skewed1)\n",
    "    data_2,parameters2 = scale_transform(data_2,skewed2)\n",
    "    data_3,parameters3 = scale_transform(data_3,skewed3)\n",
    "  \n",
    "    return (data_1[0],data_1[2]), (data_2[0],data_2[2]), (data_3[0],data_3[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
